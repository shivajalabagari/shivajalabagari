{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP9bFiH+DlZ38C9L3Kjjb1r",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shivajalabagari/shivajalabagari/blob/main/MyProject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "MEZfUdSGWybs",
        "outputId": "189d4b55-baf1-41e4-dcd2-ae09f34219ed"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-3-78c41773dea2>, line 119)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-78c41773dea2>\"\u001b[0;36m, line \u001b[0;32m119\u001b[0m\n\u001b[0;31m    df.write.partitionBy(\"postal_code\").mode(\"overwrite\").parquet(C:\\Spark_SW\\MyProject)\u001b[0m\n\u001b[0m                                                                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "# Import statements\n",
        "from pyspark.sql import SparkSession, DataFrame\n",
        "from pyspark.sql.functions import lit, col, udf, when\n",
        "from pyspark.sql.types import StringType, ArrayType, StructType, StructField, DoubleType\n",
        "from functools import reduce\n",
        "import logging\n",
        "import os\n",
        "import findspark\n",
        "\n",
        "# Spark Session\n",
        "findspark.init()\n",
        "spark = (\n",
        "    SparkSession\n",
        "    .builder\n",
        "    .appName(\"cg-pyspark-assignment\")\n",
        "    .master(\"local\")\n",
        "    .config(\"spark.sql.repl.eagerEval.enabled\", True)\n",
        "    .getOrCreate()\n",
        ")\n",
        "\n",
        "# logger\n",
        "LOGGER = logging.getLogger()\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "def get_data_by_brand(brand: str, logger: logging.Logger = LOGGER) -> DataFrame:\n",
        "\n",
        "    logger.info(f\"Loading data for brand: {brand}\")\n",
        "\n",
        "    if brand not in ['clp', 'okay', 'spar', 'dats', 'cogo']:\n",
        "        logger.error(f\"Invalid brand value: {brand}\")\n",
        "        raise ValueError(\"Allowed values are: clp, okay, spar, dats, cogo\")\n",
        "\n",
        "    file_path = f\"/content/{brand}-places.json\"\n",
        "\n",
        "    if not os.path.exists(file_path):\n",
        "        logger.error(f\"File path not exist: {file_path}\")\n",
        "        raise FileNotFoundError(f\"File path not exist: {file_path}\")\n",
        "\n",
        "    try:\n",
        "        df = spark.read.json(file_path)\n",
        "        logger.info(f\"Data loaded successfully for brand: {brand}\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error loading data for brand: {brand}: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "    # Adding Brand COlumn\n",
        "    df = df.withColumn(\"brand\", lit(brand))\n",
        "    return df\n",
        "\"\"\"Combine DF unionByName.\"\"\"\n",
        "def union_all(dfs: list) -> DataFrame:\n",
        "    return reduce(lambda df1, df2: df1.unionByName(df2), dfs)\n",
        "\n",
        "def add_missing_columns(df: DataFrame, required_cols: list) -> DataFrame:\n",
        "    \"\"\"Add missing columns to DataFrame with default values.\"\"\"\n",
        "    for col_name in required_cols:\n",
        "        if col_name not in df.columns:\n",
        "            df = df.withColumn(col_name, lit(None))\n",
        "    return df\n",
        "\n",
        "def handle_dataframes(dataframes: dict, required_cols: list) -> dict:\n",
        "\n",
        "    for name, df in dataframes.items():\n",
        "        if df is not None:\n",
        "            dataframes[name] = add_missing_columns(df, required_cols)\n",
        "    return dataframes\n",
        "\n",
        "def preprocess_data(df: DataFrame) -> DataFrame:\n",
        "\n",
        "    # Drop 'placeSearchOpeningHours'\n",
        "    if \"placeSearchOpeningHours\" in df.columns:\n",
        "        df = df.drop(\"placeSearchOpeningHours\")\n",
        "\n",
        "    # Transform 'geoCoordinates' into 'lat' and 'lon'\n",
        "    if \"geoCoordinates\" in df.columns:\n",
        "        df = df.withColumn(\"lat\", col(\"geoCoordinates.latitude\"))\n",
        "        df = df.withColumn(\"lon\", col(\"geoCoordinates.longitude\"))\n",
        "\n",
        "    # Extract 'postal_code' from 'address' using Split\n",
        "    postal_code_udf = udf(lambda address: address.split()[-1] if address else None, StringType())\n",
        "    df = df.withColumn(\"postal_code\", postal_code_udf(col(\"address\")))\n",
        "\n",
        "    # Create 'province' column on 'postal_code'\n",
        "    df = df.withColumn(\"province\", when(col(\"postal_code\").between(\"1000\", \"1299\"), \"Brussel\")\n",
        "                                    .when(col(\"postal_code\").between(\"1300\", \"1499\"), \"Waals-Brabant\")\n",
        "                                    .when(col(\"postal_code\").between(\"1500\", \"1999\"), \"Vlaams-Brabant\")\n",
        "                                    .when(col(\"postal_code\").between(\"2000\", \"2999\"), \"Antwerpen\")\n",
        "                                    .when(col(\"postal_code\").between(\"3000\", \"3499\"), \"Vlaams-Brabant\")\n",
        "                                    .when(col(\"postal_code\").between(\"3500\", \"3999\"), \"Limburg\")\n",
        "                                    .when(col(\"postal_code\").between(\"4000\", \"4999\"), \"Luik\")\n",
        "                                    .when(col(\"postal_code\").between(\"5000\", \"5999\"), \"Namen\")\n",
        "                                    .when(col(\"postal_code\").between(\"6000\", \"6599\"), \"Henegouwen\")\n",
        "                                    .when(col(\"postal_code\").between(\"7000\", \"7999\"), \"Henegouwen\")\n",
        "                                    .when(col(\"postal_code\").between(\"6600\", \"6999\"), \"Luxemburg\")\n",
        "                                    .when(col(\"postal_code\").between(\"8000\", \"8999\"), \"West-Vlaanderen\")\n",
        "                                    .when(col(\"postal_code\").between(\"9000\", \"9999\"), \"Oost-Vlaanderen\")\n",
        "                                    .otherwise(\"Unknown\"))\n",
        "\n",
        "    # Handle 'handoverServices'\n",
        "    if \"handoverServices\" in df.columns:\n",
        "        df = df.withColumn(\"handoverServices\", col(\"handoverServices\").cast(\"string\"))\n",
        "        df = df.withColumn(\"handoverServices\", when(col(\"handoverServices\").isNull(), lit(\"None\"))\n",
        "                                        .otherwise(col(\"handoverServices\")))\n",
        "\n",
        "    # Handle 'sellingPartners' if present\n",
        "    if \"sellingPartners\" in df.columns:\n",
        "        # Ensure 'sellingPartners' is of ArrayType\n",
        "        df = df.withColumn(\"sellingPartners\", col(\"sellingPartners\").cast(ArrayType(StringType())))\n",
        "        df = df.withColumn(\"sellingPartners\", when(col(\"sellingPartners\").isNull(), lit([]))\n",
        "                                            .otherwise(col(\"sellingPartners\")))\n",
        "\n",
        "    # Handle GDPR sensitive data\n",
        "    df = df.withColumn(\"houseNumber\", lit(\"ANONYMIZED\"))\n",
        "    df = df.withColumn(\"streetName\", lit(\"ANONYMIZED\"))\n",
        "\n",
        "    return df\n",
        "\n",
        "def save_data(df: DataFrame, path: str) -> None:\n",
        "\n",
        "    df.write.partitionBy(\"postal_code\").mode(\"overwrite\").parquet(C:\\Spark_SW\\MyProject)\n",
        "    LOGGER.info(f\"Data saved successfully to: {C:\\Spark_SW\\MyProject}\")\n",
        "\n",
        "# Initialize variables\n",
        "combined_df = None\n",
        "processed_df = None\n",
        "\n",
        "# List of DataFrames and their respective names\n",
        "dataframes = {\n",
        "    'clp': None,\n",
        "    'okay': None,\n",
        "    'spar': None,\n",
        "    'dats': None,\n",
        "    'cogo': None\n",
        "}\n",
        "\n",
        "# Define the required columns\n",
        "required_columns = ['handoverServices', 'sellingPartners']\n",
        "\n",
        "# Fetch DataFrames\n",
        "try:\n",
        "    for name in dataframes.keys():\n",
        "        dataframes[name] = get_data_by_brand(name)\n",
        "\n",
        "    # Print schema of each DataFrame to validate columns\n",
        "    for name, df in dataframes.items():\n",
        "        if df is not None:\n",
        "            df.printSchema()\n",
        "\n",
        "    # Ensure all DataFrames have the required columns\n",
        "    dataframes = handle_dataframes(dataframes, required_columns)\n",
        "\n",
        "    # Combine DataFrames\n",
        "    combined_df = union_all(list(dataframes.values()))\n",
        "\n",
        "    # Preprocess Data\n",
        "    processed_df = preprocess_data(combined_df)\n",
        "\n",
        "    # Save Data\n",
        "    save_data(processed_df, \"C:/Spark_SW/combined_places.parquet\")\n",
        "\n",
        "except Exception as e:\n",
        "    LOGGER.error(f\"Failed to process data: {str(e)}\")\n",
        "\n",
        "# Show sample data for each DataFrame if they were successfully loaded\n",
        "for name, df in dataframes.items():\n",
        "    if df is not None:\n",
        "        df.show(10)\n",
        "\n",
        "# Show the combined DataFrame if it was successfully created\n",
        "if combined_df is not None:\n",
        "    combined_df.show(10)\n",
        "\n",
        "# Show the processed DataFrame if it was successfully processed\n",
        "if processed_df is not None:\n",
        "    processed_df.show(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "UTsBMiUFj0pT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "LfIoKse1IbCj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "ouzo_HZNH78o"
      }
    }
  ]
}