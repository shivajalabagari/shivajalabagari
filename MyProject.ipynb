{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMC7NJbXqd8D/014OsOIcGC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shivajalabagari/shivajalabagari/blob/main/MyProject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "MEZfUdSGWybs",
        "outputId": "01646b39-c4be-481c-c351-e0b0991e9906"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'pyspark'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-360132abe9eb>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Import statements\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mudf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStringType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mArrayType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStructType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStructField\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDoubleType\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfunctools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "# Import statements\n",
        "from pyspark.sql import SparkSession, DataFrame\n",
        "from pyspark.sql.functions import lit, col, udf, when\n",
        "from pyspark.sql.types import StringType, ArrayType, StructType, StructField, DoubleType\n",
        "from functools import reduce\n",
        "import logging\n",
        "import os\n",
        "import findspark\n",
        "\n",
        "# Initialize Spark\n",
        "findspark.init()\n",
        "spark = (\n",
        "    SparkSession\n",
        "    .builder\n",
        "    .appName(\"cg-pyspark-assignment\")\n",
        "    .master(\"local\")\n",
        "    .config(\"spark.sql.repl.eagerEval.enabled\", True)\n",
        "    .getOrCreate()\n",
        ")\n",
        "\n",
        "# Initialize logger\n",
        "LOGGER = logging.getLogger()\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "def get_data_by_brand(brand: str, logger: logging.Logger = LOGGER) -> DataFrame:\n",
        "    \"\"\"Fetch input data based on brand.\n",
        "\n",
        "    Args:\n",
        "        brand: allowed values are (clp, okay, spar, dats, cogo)\n",
        "        logger: Logger object for logging\n",
        "\n",
        "    Returns:\n",
        "        The relevant DataFrame\n",
        "    \"\"\"\n",
        "    logger.info(f\"Loading data for brand: {brand}\")\n",
        "\n",
        "    if brand not in ['clp', 'okay', 'spar', 'dats', 'cogo']:\n",
        "        logger.error(f\"Invalid brand value: {brand}\")\n",
        "        raise ValueError(\"Invalid brand value. Allowed values are: clp, okay, spar, dats, cogo\")\n",
        "\n",
        "    file_path = f\"/content/{brand}-places.json\"\n",
        "\n",
        "    if not os.path.exists(file_path):\n",
        "        logger.error(f\"File path does not exist: {file_path}\")\n",
        "        raise FileNotFoundError(f\"File path does not exist: {file_path}\")\n",
        "\n",
        "    try:\n",
        "        df = spark.read.json(file_path)\n",
        "        logger.info(f\"Data loaded successfully for brand: {brand}\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error loading data for brand: {brand}: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "    # Add brand column\n",
        "    df = df.withColumn(\"brand\", lit(brand))\n",
        "    return df\n",
        "\n",
        "def union_all(dfs: list) -> DataFrame:\n",
        "    \"\"\"Combine multiple DataFrames into a single DataFrame using unionByName.\"\"\"\n",
        "    return reduce(lambda df1, df2: df1.unionByName(df2), dfs)\n",
        "\n",
        "def add_missing_columns(df: DataFrame, required_cols: list) -> DataFrame:\n",
        "    \"\"\"Add missing columns to DataFrame with default values.\"\"\"\n",
        "    for col_name in required_cols:\n",
        "        if col_name not in df.columns:\n",
        "            df = df.withColumn(col_name, lit(None))\n",
        "    return df\n",
        "\n",
        "def handle_dataframes(dataframes: dict, required_cols: list) -> dict:\n",
        "    \"\"\"Ensure all DataFrames have the required columns.\"\"\"\n",
        "    for name, df in dataframes.items():\n",
        "        if df is not None:\n",
        "            dataframes[name] = add_missing_columns(df, required_cols)\n",
        "    return dataframes\n",
        "\n",
        "def preprocess_data(df: DataFrame) -> DataFrame:\n",
        "    \"\"\"Process DataFrame according to the given requirements.\n",
        "\n",
        "    Args:\n",
        "        df: The input DataFrame.\n",
        "\n",
        "    Returns:\n",
        "        The processed DataFrame.\n",
        "    \"\"\"\n",
        "    # Drop 'placeSearchOpeningHours'\n",
        "    if \"placeSearchOpeningHours\" in df.columns:\n",
        "        df = df.drop(\"placeSearchOpeningHours\")\n",
        "\n",
        "    # Transform 'geoCoordinates' into 'lat' and 'lon'\n",
        "    if \"geoCoordinates\" in df.columns:\n",
        "        df = df.withColumn(\"lat\", col(\"geoCoordinates.latitude\"))\n",
        "        df = df.withColumn(\"lon\", col(\"geoCoordinates.longitude\"))\n",
        "\n",
        "    # Extract 'postal_code' from 'address'\n",
        "    postal_code_udf = udf(lambda address: address.split()[-1] if address else None, StringType())\n",
        "    df = df.withColumn(\"postal_code\", postal_code_udf(col(\"address\")))\n",
        "\n",
        "    # Create 'province' column based on 'postal_code'\n",
        "    df = df.withColumn(\"province\", when(col(\"postal_code\").between(\"1000\", \"1299\"), \"Brussel\")\n",
        "                                    .when(col(\"postal_code\").between(\"1300\", \"1499\"), \"Waals-Brabant\")\n",
        "                                    .when(col(\"postal_code\").between(\"1500\", \"1999\"), \"Vlaams-Brabant\")\n",
        "                                    .when(col(\"postal_code\").between(\"2000\", \"2999\"), \"Antwerpen\")\n",
        "                                    .when(col(\"postal_code\").between(\"3000\", \"3499\"), \"Vlaams-Brabant\")\n",
        "                                    .when(col(\"postal_code\").between(\"3500\", \"3999\"), \"Limburg\")\n",
        "                                    .when(col(\"postal_code\").between(\"4000\", \"4999\"), \"Luik\")\n",
        "                                    .when(col(\"postal_code\").between(\"5000\", \"5999\"), \"Namen\")\n",
        "                                    .when(col(\"postal_code\").between(\"6000\", \"6599\"), \"Henegouwen\")\n",
        "                                    .when(col(\"postal_code\").between(\"7000\", \"7999\"), \"Henegouwen\")\n",
        "                                    .when(col(\"postal_code\").between(\"6600\", \"6999\"), \"Luxemburg\")\n",
        "                                    .when(col(\"postal_code\").between(\"8000\", \"8999\"), \"West-Vlaanderen\")\n",
        "                                    .when(col(\"postal_code\").between(\"9000\", \"9999\"), \"Oost-Vlaanderen\")\n",
        "                                    .otherwise(\"Unknown\"))\n",
        "\n",
        "    # Handle 'handoverServices'\n",
        "    if \"handoverServices\" in df.columns:\n",
        "        df = df.withColumn(\"handoverServices\", col(\"handoverServices\").cast(\"string\"))\n",
        "        df = df.withColumn(\"handoverServices\", when(col(\"handoverServices\").isNull(), lit(\"None\"))\n",
        "                                        .otherwise(col(\"handoverServices\")))\n",
        "\n",
        "    # Handle 'sellingPartners' if present\n",
        "    if \"sellingPartners\" in df.columns:\n",
        "        # Ensure 'sellingPartners' is of ArrayType\n",
        "        df = df.withColumn(\"sellingPartners\", col(\"sellingPartners\").cast(ArrayType(StringType())))\n",
        "        df = df.withColumn(\"sellingPartners\", when(col(\"sellingPartners\").isNull(), lit([]))\n",
        "                                            .otherwise(col(\"sellingPartners\")))\n",
        "\n",
        "    # Handle GDPR sensitive data\n",
        "    df = df.withColumn(\"houseNumber\", lit(\"ANONYMIZED\"))\n",
        "    df = df.withColumn(\"streetName\", lit(\"ANONYMIZED\"))\n",
        "\n",
        "    return df\n",
        "\n",
        "def save_data(df: DataFrame, path: str) -> None:\n",
        "    \"\"\"Save DataFrame to parquet file with partitioning.\n",
        "\n",
        "    Args:\n",
        "        df: The DataFrame to save.\n",
        "        path: The path where the parquet file will be saved.\n",
        "    \"\"\"\n",
        "    df.write.partitionBy(\"postal_code\").mode(\"overwrite\").parquet(path)\n",
        "    LOGGER.info(f\"Data saved successfully to: {path}\")\n",
        "\n",
        "# Initialize variables\n",
        "combined_df = None\n",
        "processed_df = None\n",
        "\n",
        "# List of DataFrames and their respective names\n",
        "dataframes = {\n",
        "    'clp': None,\n",
        "    'okay': None,\n",
        "    'spar': None,\n",
        "    'dats': None,\n",
        "    'cogo': None\n",
        "}\n",
        "\n",
        "# Define the required columns\n",
        "required_columns = ['handoverServices', 'sellingPartners']\n",
        "\n",
        "# Fetch DataFrames\n",
        "try:\n",
        "    for name in dataframes.keys():\n",
        "        dataframes[name] = get_data_by_brand(name)\n",
        "\n",
        "    # Print schema of each DataFrame to validate columns\n",
        "    for name, df in dataframes.items():\n",
        "        if df is not None:\n",
        "            df.printSchema()\n",
        "\n",
        "    # Ensure all DataFrames have the required columns\n",
        "    dataframes = handle_dataframes(dataframes, required_columns)\n",
        "\n",
        "    # Combine DataFrames\n",
        "    combined_df = union_all(list(dataframes.values()))\n",
        "\n",
        "    # Preprocess Data\n",
        "    processed_df = preprocess_data(combined_df)\n",
        "\n",
        "    # Save Data\n",
        "    save_data(processed_df, \"C:/Spark_SW/combined_places.parquet\")\n",
        "\n",
        "except Exception as e:\n",
        "    LOGGER.error(f\"Failed to process data: {str(e)}\")\n",
        "\n",
        "# Show sample data for each DataFrame if they were successfully loaded\n",
        "for name, df in dataframes.items():\n",
        "    if df is not None:\n",
        "        df.show(5)\n",
        "\n",
        "# Show the combined DataFrame if it was successfully created\n",
        "if combined_df is not None:\n",
        "    combined_df.show(5)\n",
        "\n",
        "# Show the processed DataFrame if it was successfully processed\n",
        "if processed_df is not None:\n",
        "    processed_df.show(5)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "UTsBMiUFj0pT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "LfIoKse1IbCj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "ouzo_HZNH78o"
      }
    }
  ]
}